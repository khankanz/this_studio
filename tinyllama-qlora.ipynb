{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA with TinyLlama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templating Instruction Data\n",
    "To have the LLM follow instructions, we will need to prepare instruction data that follows a chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_tokz = AutoTokenizer.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically message format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n[ { \"content\": \"These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\\n\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme\\'s built-in \\nsettings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\n\\nDoes this feature apply to all sections of the theme or just specific ones as listed in the text material?\", \"role\": \"user\" }, \\n{ \"content\": \"This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.\", \\n\"role\": \"assistant\" }, { \"content\": \"Can you guide me through the process of enabling the secondary image hover feature on my \\nCollection pages and Featured Collections sections?\", \"role\": \"user\" }, \\n{ \"content\": \"Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section\\'s setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled \\'Image display\\' or \\'Image hover\\'.\\n6. If available, select \\'Show secondary image on hover\\'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you\\'re having trouble finding the setting, the best thing to do is refer to your theme\\'s documentation, since the location and labeling of settings can vary between themes.\", \"role\": \"assistant\" }, { \"content\": \"Can you provide me with a link to the documentation for my theme?\", \"role\": \"user\" }, { \"content\": \"I don\\'t have access to your store\\'s theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the \\'support\\' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by \\'documentation\\' or \\'user guide\\'.\", \"role\": \"assistant\" }, { \"content\": \"Can you confirm if this feature also works for the Quick Shop section of my theme?\", \"role\": \"user\" }, { \"content\": \"The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for \\'Image display\\' or \\'Image hover\\'. 3. If available, select \\'Show secondary image on hover\\'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.\", \"role\": \"assistant\" } ]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "[ { \"content\": \"These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\n",
    "\\nOn your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in \n",
    "settings!\\nYour Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\\n\n",
    "Does this feature apply to all sections of the theme or just specific ones as listed in the text material?\", \"role\": \"user\" }, \n",
    "{ \"content\": \"This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.\", \n",
    "\"role\": \"assistant\" }, { \"content\": \"Can you guide me through the process of enabling the secondary image hover feature on my \n",
    "Collection pages and Featured Collections sections?\", \"role\": \"user\" }, \n",
    "{ \"content\": \"Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\\n\\n1. Log in to your Shopify account and go to your Online Store.\\n2. Click on Customize theme for the section-based theme you are using.\\n3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\\n4. With the section open, click on the section's setting (gear) icon in the top-left corner.\\n5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\\n6. If available, select 'Show secondary image on hover'.\\n7. Save the changes and preview the Collection/Featured Collection page to see the effect.\\n\\nIf you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.\", \"role\": \"assistant\" }, { \"content\": \"Can you provide me with a link to the documentation for my theme?\", \"role\": \"user\" }, { \"content\": \"I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.\", \"role\": \"assistant\" }, { \"content\": \"Can you confirm if this feature also works for the Quick Shop section of my theme?\", \"role\": \"user\" }, { \"content\": \"The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\\n\\n1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.\", \"role\": \"assistant\" } ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's write a format prompt function to turn our df into this message format for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_prompt(row):\n",
    "    \"\"\"Format the prompt to using the <|user|> template TinyLlama is using.\"\"\"\n",
    "    report_text = row['report_text']\n",
    "    label = row['label']\n",
    "    message = [{'role': 'user',\n",
    "    'content': f\"In the following radiology report, classify the patient's current microcalcification status as Positive, Negative or Not Stated. {report_text}\"}, \n",
    "    {'role':'assistant', 'content': label}]\n",
    "    prompt = template_tokz.apply_chat_template(message, tokenize=False)\n",
    "    return {'text': prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = {'report_text': \"\"\"BILATERAL SCREENING MAMMOGRAPHY.\n",
    "History: Screening.\n",
    "Comparison available dating from 01/2023.\n",
    "Findings:\n",
    "There are scattered fibroglandular densities bilaterally. No skin thickening or nipple retraction is seen. No grouped calcifications are identified. No spiculated or circumscribed masses are seen.\n",
    "IMPRESSION:\n",
    "No mammographic evidence of malignancy. BI-RADS Category 1.\"\"\", 'label': 'Negative'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<|user|>\\nIn the following radiology report, classify the patient's current microcalcification status as Positive, Negative or Not Stated. BILATERAL SCREENING MAMMOGRAPHY.\\nHistory: Screening.\\nComparison available dating from 01/2023.\\nFindings:\\nThere are scattered fibroglandular densities bilaterally. No skin thickening or nipple retraction is seen. No grouped calcifications are identified. No spiculated or circumscribed masses are seen.\\nIMPRESSION:\\nNo mammographic evidence of malignancy. BI-RADS Category 1.</s>\\n<|assistant|>\\nNegative</s>\\n\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmt_prompt(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay let's load our synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_PATH = Path.cwd()/'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REPORT:\\n\"BILATERAL SCREENING MAMMOGRAM\\nCLINI...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCREENING MAMMOGRAM.\\nCompared to Previous: Ye...</td>\n",
       "      <td>Not Stated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         report_text       label\n",
       "0  REPORT:\\n\"BILATERAL SCREENING MAMMOGRAM\\nCLINI...    Positive\n",
       "1  SCREENING MAMMOGRAM.\\nCompared to Previous: Ye...  Not Stated"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DS_PATH/'synth-train-n1132.csv')\n",
    "df = df.drop(columns=['Unnamed: 0']) # Let's remove this column from my df\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['report_text', 'label'],\n",
       "    num_rows: 1132\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(df) ; ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607ec597aeeb460580b91114334ce13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(fmt_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|user|>\\nIn the following radiology report, classify the patient's current microcalcification status as Positive, Negative or Not Stated. BILATERAL SCREENING MAMMOGRAPHY\\nHistory: Screening mammogram.\\nComparison available dating from 02/2022.\\nFindings:\\nThere are scattered fibroglandular densities bilaterally. No skin thickening or nipple retraction is seen. No grouped calcifications are identified. No spiculated or circumscribed masses are seen.\\nIMPRESSION:\\nNo mammographic evidence of malignancy. BI-RADS Category 1.</s>\\n<|assistant|>\\nNegative</s>\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['text'][55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Quantization\n",
    "> Using bitsandbytes package to compress the pretrained model to a 4-bit quantization. Following the QLoRA paper and load the model in `4-bit`, normalized float representation and double quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the intermediate step? I'm not sure rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = 'TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization config - Q in QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # using 4-bit precision model loading\n",
    "    bnb_4bit_quant_type='nf4', # quantization type\n",
    "    bnb_4bit_compute_dtype='float16', #compute dtype\n",
    "    bnb_4bit_use_double_quant=True # Apply nested quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    mn,\n",
    "    device_map='auto',\n",
    "    quantization_config=bnb_config # leave this out for regular SFT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMa Tokz\n",
    "tokz = AutoTokenizer.from_pretrained(mn, trust_remote_code=True)\n",
    "tokz.pad_token=\"<PAD>\"\n",
    "tokz.padding_side='left'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This quantization procedure allows us to decrease the size of the original model while retaining most of hte orignal weights' precision. Loading the model now requires ~1GB VRAM compared to the 4GB of VRAM it would need without quantization. Note that during fine-tuning, more VRAM will be necessary so it does not cap out on the 1GB VRAM needed to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We need to define our LoRA configuration using the `peft` library which represents hyperparameters of the ft\"\"\"\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few questions for myself: how do we know about target modules? and how to set these settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32, # LoRA scaling\n",
    "    lora_dropout=0.1, # dropout for LoRA layers\n",
    "    r=64, # rank\n",
    "    bias=\"none\",\n",
    "    task_type='CAUSAL_LM',\n",
    "    target_modules= [\"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \"o_proj\", \"down_proj\"] #layers to target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`r`\n",
    "This is the rank of the compressed matrices (recall this from Figure 12-13) Increasing this value will also increase the sizes of compressed matrices leading to less compression and thereby improved representative power. Values typically range between 4 and 64.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lora_alpha`\n",
    "Controls the amount of change that is added to the original weights. In essence, it balances the knowledge of the original model with that of the new task. A rule of thumb is to choose a value twice the size of r.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`target_modules`\n",
    "Controls which layers to target. The LoRA procedure can choose to ignore specific layers, like specific projection layers. This can speed up training but reduce performance and vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/teamspace/studios/this_studio/models')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = Path.cwd()/'models' ; output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "lr = 2e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_train_epochs`\n",
    "The total number of training rounds. Higher values tend to degrade performance so we generally like to keep this low.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`learning_rate`\n",
    "Determines the step size at each iteration of weight updates. The authors of QLoRA found that higher learning rates work better for larger models (>33B parameters).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lr_scheduler_type`\n",
    "A cosine-based scheduler to adjust the learning rate dynamically. It will linearly increase the learning rate, starting from zero, until it reaches the set value. After that, the learning rate is decayed following the values of a cosine function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`optim`\n",
    "The paged optimizers used in the original QLoRA paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=bs,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim='paged_adamw_32bit',\n",
    "    learning_rate=lr,\n",
    "    lr_scheduler_type='cosine',\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not HF Trainer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b4cd35dfac42a0966c41a538503532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokz,\n",
    "    args=training_arguments,\n",
    "    max_seq_length=512,\n",
    "\n",
    "    # Leave this out for regular SFT\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='141' max='141' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [141/141 03:33, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.869900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.922700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.850600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.767000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.772100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.663200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.733700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.682200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.698800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.663200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.623900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=141, training_loss=0.8366218462903449, metrics={'train_runtime': 215.0656, 'train_samples_per_second': 5.264, 'train_steps_per_second': 0.656, 'total_flos': 2645407998074880.0, 'train_loss': 0.8366218462903449, 'epoch': 0.9964664310954063})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"TinyLlama-1.1B-qlora\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Weights\n",
    "> fter we have trained our QLoRA weights, we still need to combine them with the original weights to use them. We reload the model in 16 bits, instead of the quantized 4 bits, to merge the weights. Although the tokenizer was not updated during training, we save it to the same folder as the model for easier access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    'TinyLlama-1.1B-qlora',\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging the adapter with base model, we can use it with the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<|user|>\n",
    "In the following radiology report, classify the patient's current microcalcification status as Positive, Negative or Not Stated. Microcalcifications right breast with underlying dilated ducts, most of which are related to milk of calcium within microcysts and some indeterminate. Ultrasound guided core biopsy in the area of the palpable lump containing the microcalcifications will be done done at 6:00 today.</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "In the following radiology report, classify the patient's current microcalcification status as Positive, Negative or Not Stated. Microcalcifications right breast with underlying dilated ducts, most of which are related to milk of calcium within microcysts and some indeterminate. Ultrasound guided core biopsy in the area of the palpable lump containing the microcalcifications will be done done at 6:00 today.</s>\n",
      "<|assistant|>\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# Run our instruction-tuned model\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokz)\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay I expected this to output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
